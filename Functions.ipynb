{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:01:56.779536Z",
     "start_time": "2024-02-26T15:01:55.796981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 0, 1, 1, 1, 1, 1],\n        [1, 0, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1],\n        [1, 0, 1, 0, 1, 1, 1],\n        [0, 1, 1, 1, 1, 1, 1],\n        [1, 0, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original matrix provided by the user\n",
    "matrix = torch.tensor([[1, 1, 1, 1, 0, 1, 1],\n",
    "                         [0, 0, 1, 0, 1, 0, 1],\n",
    "                         [1, 1, 1, 1, 1, 1, 1],\n",
    "                         [1, 1, 1, 0, 1, 1, 1],\n",
    "                         [1, 1, 1, 1, 1, 1, 1],\n",
    "                         [1, 1, 1, 1, 1, 1, 1],\n",
    "                         [1, 1, 1, 1, 1, 1, 1],], dtype=torch.int)\n",
    "\n",
    "\n",
    "# Compute the transpose of the matrix\n",
    "transposed_matrix = matrix.t()\n",
    "\n",
    "transposed_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n         1, 1, 1, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n         1, 0, 1, 1],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n         1, 1, 1, 1],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n         1, 1, 1, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n         0, 1, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n         0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n         1, 1, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n         1, 0, 0, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n         0, 1, 0, 1]]),\n tensor([[1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n          0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          1., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 1., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 1., 0., 0., 0., 0., 0.],\n         [1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 1., 0., 0., 0., 0.],\n         [0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 1., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 1., 0.],\n         [0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 1.]]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define dimensions\n",
    "k, n = 10, 26\n",
    "r = n - k\n",
    "\n",
    "# Construct G\n",
    "G_identity = np.eye(k, dtype=int)\n",
    "G_parity = np.random.randint(0, 2, size=(k, r))\n",
    "G = np.hstack((G_identity, G_parity))\n",
    "\n",
    "# # Construct H by finding nullspace of G\n",
    "# H = np.linalg.null_space(G.T)\n",
    "\n",
    "G\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:03:32.905722Z",
     "start_time": "2024-02-08T13:03:32.895187Z"
    }
   },
   "id": "623c9a045aea3b08",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6f6747a3decaf13d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 22\u001B[0m\n\u001B[1;32m     11\u001B[0m tensor2_reshaped \u001B[38;5;241m=\u001B[39m tensor2\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Broadcast tensor2 across the rows of tensor1 and perform element-wise division\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Note: tensor1 needs to be divided in a manner that matches your description, which seems to have a misunderstanding.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# If you intend to divide each column by a single value, tensor2 should represent a value per column, not per row.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m \n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Perform the division\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtensor1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtensor2_reshaped\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResulting Tensor:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, result)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (7) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your first tensor\n",
    "tensor1 = torch.tensor([[0.1582, 0.2170, 0.1495, 0.3974, 0.4741, 0.2054, 0.7275],\n",
    "                        [0.8117, 0.4013, 0.1386, 0.2082, 0.2137, 0.1977, 0.0406]])\n",
    "\n",
    "# Your second tensor, which seems to be intended as a per-column property\n",
    "tensor2 = torch.tensor([2.3291, 2.0119])\n",
    "\n",
    "# Reshape tensor2 from [2] to [1, 2] for proper broadcasting\n",
    "tensor2_reshaped = tensor2.view(1, -1)\n",
    "\n",
    "# Broadcast tensor2 across the rows of tensor1 and perform element-wise division\n",
    "# Note: tensor1 needs to be divided in a manner that matches your description, which seems to have a misunderstanding.\n",
    "# If you intend to divide each column by a single value, tensor2 should represent a value per column, not per row.\n",
    "# Assuming a correction, where tensor2 represents column-wise properties and has shape [7], here's a corrected approach:\n",
    "\n",
    "# Corrected tensor2 for column-wise properties\n",
    "# tensor2_corrected = torch.tensor([2.3291, 2.0119])  # Placeholder, replace with actual column-wise values\n",
    "\n",
    "# Perform the division\n",
    "result = tensor1 / tensor2_reshaped\n",
    "\n",
    "print(\"Resulting Tensor:\\n\", result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T05:03:06.522649Z",
     "start_time": "2024-02-22T05:03:06.499Z"
    }
   },
   "id": "fe1d9a51737f0658",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,\n",
      "        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,\n",
      "        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,\n",
      "        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,\n",
      "        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,\n",
      "        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0500, 0.0510, 0.0520, 0.0530,\n",
      "        0.0540, 0.0550, 0.0560, 0.0570, 0.0580, 0.0590, 0.0600, 0.0610, 0.0620,\n",
      "        0.0630, 0.0640, 0.0650, 0.0660, 0.0670, 0.0680, 0.0690, 0.0700, 0.0710,\n",
      "        0.0720, 0.0730, 0.0740, 0.0750, 0.0760, 0.0770, 0.0780, 0.0790, 0.0800,\n",
      "        0.0810, 0.0820, 0.0830, 0.0840, 0.0850, 0.0860, 0.0870, 0.0880, 0.0890,\n",
      "        0.0900, 0.0910, 0.0920, 0.0930, 0.0940, 0.0950, 0.0960, 0.0970, 0.0980,\n",
      "        0.0990, 0.1000, 0.1010, 0.1020, 0.1030, 0.1040, 0.1050, 0.1060, 0.1070,\n",
      "        0.1080, 0.1090, 0.1100, 0.1110, 0.1120, 0.1130, 0.1140, 0.1150, 0.1160,\n",
      "        0.1170, 0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250,\n",
      "        0.1260, 0.1270, 0.1280, 0.1290, 0.1300, 0.1310, 0.1320, 0.1330, 0.1340,\n",
      "        0.1350, 0.1360, 0.1370, 0.1380, 0.1390, 0.1400, 0.1410, 0.1420, 0.1430,\n",
      "        0.1440, 0.1450, 0.1460, 0.1470, 0.1480, 0.1490, 0.1500, 0.1510, 0.1520,\n",
      "        0.1530, 0.1540, 0.1550, 0.1560, 0.1570, 0.1580, 0.1590, 0.1600, 0.1610,\n",
      "        0.1620, 0.1630, 0.1640, 0.1650, 0.1660, 0.1670, 0.1680, 0.1690, 0.1700,\n",
      "        0.1710, 0.1720, 0.1730, 0.1740, 0.1750, 0.1760, 0.1770, 0.1780, 0.1790,\n",
      "        0.1800, 0.1810, 0.1820, 0.1830, 0.1840, 0.1850, 0.1860, 0.1870, 0.1880,\n",
      "        0.1890, 0.1900, 0.1910, 0.1920, 0.1930, 0.1940, 0.1950, 0.1960, 0.1970,\n",
      "        0.1980, 0.1990, 0.2000, 0.2010, 0.2020, 0.2030, 0.2040, 0.2050, 0.2060,\n",
      "        0.2070, 0.2080, 0.2090, 0.2100, 0.2110, 0.2120, 0.2130, 0.2140, 0.2150,\n",
      "        0.2160, 0.2170, 0.2180, 0.2190, 0.2200, 0.2210, 0.2220, 0.2230, 0.2240,\n",
      "        0.2250, 0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320, 0.2330,\n",
      "        0.2340, 0.2350, 0.2360, 0.2370, 0.2380, 0.2390, 0.2400, 0.2410, 0.2420,\n",
      "        0.2430, 0.2440, 0.2450, 0.2460, 0.2470, 0.2480, 0.2490, 0.2500, 0.2510,\n",
      "        0.2520, 0.2530, 0.2540, 0.2550, 0.2560, 0.2570, 0.2580, 0.2590, 0.2600,\n",
      "        0.2610, 0.2620, 0.2630, 0.2640, 0.2650, 0.2660, 0.2670, 0.2680, 0.2690,\n",
      "        0.2700, 0.2710, 0.2720, 0.2730, 0.2740, 0.2750, 0.2760, 0.2770, 0.2780,\n",
      "        0.2790, 0.2800, 0.2810, 0.2820, 0.2830, 0.2840, 0.2850, 0.2860, 0.2870,\n",
      "        0.2880, 0.2890, 0.2900, 0.2910, 0.2920, 0.2930, 0.2940, 0.2950, 0.2960,\n",
      "        0.2970, 0.2980, 0.2990, 0.3000, 0.3010, 0.3020, 0.3030, 0.3040, 0.3050,\n",
      "        0.3060, 0.3070, 0.3080, 0.3090, 0.3100, 0.3110, 0.3120, 0.3130, 0.3140,\n",
      "        0.3150, 0.3160, 0.3170, 0.3180, 0.3190, 0.3200, 0.3210, 0.3220, 0.3230,\n",
      "        0.3240, 0.3250, 0.3260, 0.3270, 0.3280, 0.3290, 0.3300, 0.3310, 0.3320,\n",
      "        0.3330, 0.3340, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390, 0.3400, 0.3410,\n",
      "        0.3420, 0.3430, 0.3440, 0.3450, 0.3460, 0.3470, 0.3480, 0.3490, 0.3500,\n",
      "        0.3510, 0.3520, 0.3530, 0.3540, 0.3550, 0.3560, 0.3570, 0.3580, 0.3590,\n",
      "        0.3600, 0.3610, 0.3620, 0.3630, 0.3640, 0.3650, 0.3660, 0.3670, 0.3680,\n",
      "        0.3690, 0.3700, 0.3710, 0.3720, 0.3730, 0.3740, 0.3750, 0.3760, 0.3770,\n",
      "        0.3780, 0.3790, 0.3800, 0.3810, 0.3820, 0.3830, 0.3840, 0.3850, 0.3860,\n",
      "        0.3870, 0.3880, 0.3890, 0.3900, 0.3910, 0.3920, 0.3930, 0.3940, 0.3950,\n",
      "        0.3960, 0.3970, 0.3980, 0.3990, 0.4000, 0.4010, 0.4020, 0.4030, 0.4040,\n",
      "        0.4050, 0.4060, 0.4070, 0.4080, 0.4090, 0.4100, 0.4110, 0.4120, 0.4130,\n",
      "        0.4140, 0.4150, 0.4160, 0.4170, 0.4180, 0.4190, 0.4200, 0.4210, 0.4220,\n",
      "        0.4230, 0.4240, 0.4250, 0.4260, 0.4270, 0.4280, 0.4290, 0.4300, 0.4310,\n",
      "        0.4320, 0.4330, 0.4340, 0.4350, 0.4360, 0.4370, 0.4380, 0.4390, 0.4400,\n",
      "        0.4410, 0.4420, 0.4430, 0.4440, 0.4450, 0.4460, 0.4470, 0.4480, 0.4490,\n",
      "        0.4500, 0.4510, 0.4520, 0.4530, 0.4540, 0.4550, 0.4560, 0.4570, 0.4580,\n",
      "        0.4590, 0.4600, 0.4610, 0.4620, 0.4630, 0.4640, 0.4650, 0.4660, 0.4670,\n",
      "        0.4680, 0.4690, 0.4700, 0.4710, 0.4720, 0.4730, 0.4740, 0.4750, 0.4760,\n",
      "        0.4770, 0.4780, 0.4790, 0.4800, 0.4810, 0.4820, 0.4830, 0.4840, 0.4850,\n",
      "        0.4860, 0.4870, 0.4880, 0.4890, 0.4900, 0.4910, 0.4920, 0.4930, 0.4940,\n",
      "        0.4950, 0.4960, 0.4970, 0.4980, 0.4990, 0.5000, 0.5010, 0.5020, 0.5030,\n",
      "        0.5040, 0.5050, 0.5060, 0.5070, 0.5080, 0.5090, 0.5100, 0.5110, 0.5120,\n",
      "        0.5130, 0.5140, 0.5150, 0.5160, 0.5170, 0.5180, 0.5190, 0.5200, 0.5210,\n",
      "        0.5220, 0.5230, 0.5240, 0.5250, 0.5260, 0.5270, 0.5280, 0.5290, 0.5300,\n",
      "        0.5310, 0.5320, 0.5330, 0.5340, 0.5350, 0.5360, 0.5370, 0.5380, 0.5390,\n",
      "        0.5400, 0.5410, 0.5420, 0.5430, 0.5440, 0.5450, 0.5460, 0.5470, 0.5480,\n",
      "        0.5490, 0.5500, 0.5510, 0.5520, 0.5530, 0.5540, 0.5550, 0.5560, 0.5570,\n",
      "        0.5580, 0.5590, 0.5600, 0.5610, 0.5620, 0.5630, 0.5640, 0.5650, 0.5660,\n",
      "        0.5670, 0.5680, 0.5690, 0.5700, 0.5710, 0.5720, 0.5730, 0.5740, 0.5750,\n",
      "        0.5760, 0.5770, 0.5780, 0.5790, 0.5800, 0.5810, 0.5820, 0.5830, 0.5840,\n",
      "        0.5850, 0.5860, 0.5870, 0.5880, 0.5890, 0.5900, 0.5910, 0.5920, 0.5930,\n",
      "        0.5940, 0.5950, 0.5960, 0.5970, 0.5980, 0.5990, 0.6000, 0.6010, 0.6020,\n",
      "        0.6030, 0.6040, 0.6050, 0.6060, 0.6070, 0.6080, 0.6090, 0.6100, 0.6110,\n",
      "        0.6120, 0.6130, 0.6140, 0.6150, 0.6160, 0.6170, 0.6180, 0.6190, 0.6200,\n",
      "        0.6210, 0.6220, 0.6230, 0.6240, 0.6250, 0.6260, 0.6270, 0.6280, 0.6290,\n",
      "        0.6300, 0.6310, 0.6320, 0.6330, 0.6340, 0.6350, 0.6360, 0.6370, 0.6380,\n",
      "        0.6390, 0.6400, 0.6410, 0.6420, 0.6430, 0.6440, 0.6450, 0.6460, 0.6470,\n",
      "        0.6480, 0.6490, 0.6500, 0.6510, 0.6520, 0.6530, 0.6540, 0.6550, 0.6560,\n",
      "        0.6570, 0.6580, 0.6590, 0.6600, 0.6610, 0.6620, 0.6630, 0.6640, 0.6650,\n",
      "        0.6660, 0.6670, 0.6680, 0.6690, 0.6700, 0.6710, 0.6720, 0.6730, 0.6740,\n",
      "        0.6750, 0.6760, 0.6770, 0.6780, 0.6790, 0.6800, 0.6810, 0.6820, 0.6830,\n",
      "        0.6840, 0.6850, 0.6860, 0.6870, 0.6880, 0.6890, 0.6900, 0.6910, 0.6920,\n",
      "        0.6930, 0.6940, 0.6950, 0.6960, 0.6970, 0.6980, 0.6990, 0.7000, 0.7010,\n",
      "        0.7020, 0.7030, 0.7040, 0.7050, 0.7060, 0.7070, 0.7080, 0.7090, 0.7100,\n",
      "        0.7110, 0.7120, 0.7130, 0.7140, 0.7150, 0.7160, 0.7170, 0.7180, 0.7190,\n",
      "        0.7200, 0.7210, 0.7220, 0.7230, 0.7240, 0.7250, 0.7260, 0.7270, 0.7280,\n",
      "        0.7290, 0.7300, 0.7310, 0.7320, 0.7330, 0.7340, 0.7350, 0.7360, 0.7370,\n",
      "        0.7380, 0.7390, 0.7400, 0.7410, 0.7420, 0.7430, 0.7440, 0.7450, 0.7460,\n",
      "        0.7470, 0.7480, 0.7490, 0.7500, 0.7510, 0.7520, 0.7530, 0.7540, 0.7550,\n",
      "        0.7560, 0.7570, 0.7580, 0.7590, 0.7600, 0.7610, 0.7620, 0.7630, 0.7640,\n",
      "        0.7650, 0.7660, 0.7670, 0.7680, 0.7690, 0.7700, 0.7710, 0.7720, 0.7730,\n",
      "        0.7740, 0.7750, 0.7760, 0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820,\n",
      "        0.7830, 0.7840, 0.7850, 0.7860, 0.7870, 0.7880, 0.7890, 0.7900, 0.7910,\n",
      "        0.7920, 0.7930, 0.7940, 0.7950, 0.7960, 0.7970, 0.7980, 0.7990, 0.8000,\n",
      "        0.8010, 0.8020, 0.8030, 0.8040, 0.8050, 0.8060, 0.8070, 0.8080, 0.8090,\n",
      "        0.8100, 0.8110, 0.8120, 0.8130, 0.8140, 0.8150, 0.8160, 0.8170, 0.8180,\n",
      "        0.8190, 0.8200, 0.8210, 0.8220, 0.8230, 0.8240, 0.8250, 0.8260, 0.8270,\n",
      "        0.8280, 0.8290, 0.8300, 0.8310, 0.8320, 0.8330, 0.8340, 0.8350, 0.8360,\n",
      "        0.8370, 0.8380, 0.8390, 0.8400, 0.8410, 0.8420, 0.8430, 0.8440, 0.8450,\n",
      "        0.8460, 0.8470, 0.8480, 0.8490, 0.8500, 0.8510, 0.8520, 0.8530, 0.8540,\n",
      "        0.8550, 0.8560, 0.8570, 0.8580, 0.8590, 0.8600, 0.8610, 0.8620, 0.8630,\n",
      "        0.8640, 0.8650, 0.8660, 0.8670, 0.8680, 0.8690, 0.8700, 0.8710, 0.8720,\n",
      "        0.8730, 0.8740, 0.8750, 0.8760, 0.8770, 0.8780, 0.8790, 0.8800, 0.8810,\n",
      "        0.8820, 0.8830, 0.8840, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900,\n",
      "        0.8910, 0.8920, 0.8930, 0.8940, 0.8950, 0.8960, 0.8970, 0.8980, 0.8990])\n"
     ]
    }
   ],
   "source": [
    "threshold = torch.arange(0, 0.9, 0.001)\n",
    "print(threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T06:31:21.353677Z",
     "start_time": "2024-02-24T06:31:21.344196Z"
    }
   },
   "id": "ce539bbe30ebdb58",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified model: OrderedDict([('hidden.weight', tensor([[ 0.0000,  0.0000,  0.7195,  0.0000,  0.0000,  0.7038,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.7659,  0.0000,  0.7190,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1143,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8998],\n",
      "        [ 0.0000,  1.2054,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.6529,  0.0000,  0.6603,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.6563,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])), ('hidden.bias', tensor([2.5140, 2.6826, 2.6397, 2.6925, 2.5278, 2.5654, 2.7825])), ('output.weight', tensor([[-0.9320, -1.2707,  1.1320,  1.0465, -0.8846, -0.6712,  0.6520],\n",
      "        [-1.0896,  0.8015,  0.9044,  0.2587,  0.7822,  0.9356, -1.4586],\n",
      "        [-0.8282,  0.9302, -0.0997,  0.9627,  1.0229, -0.7685,  1.1151],\n",
      "        [ 0.8348,  0.7909,  0.9691,  0.4444, -1.1093, -0.9300, -1.3207],\n",
      "        [-0.9278, -0.3306, -1.2269,  1.3560, -0.8565,  1.2990, -0.5990],\n",
      "        [-0.9841, -1.1325,  0.1131, -0.9265,  0.9693, -0.7067, -1.0114],\n",
      "        [ 0.9285, -0.5912, -1.2709,  1.2431,  0.9091, -0.7206, -0.9369],\n",
      "        [-0.8331,  1.2798, -1.1358, -0.7052, -0.7538, -0.6297,  0.0042],\n",
      "        [ 0.8271, -1.5606,  0.9740,  0.6647,  0.7538,  1.0012, -0.0026],\n",
      "        [-0.9326,  0.3381,  1.1142, -1.3098, -0.9829,  1.1082,  0.9461],\n",
      "        [ 0.9843,  0.8563, -0.2729,  0.8246, -0.8743,  1.0507,  0.8982],\n",
      "        [ 0.9551,  0.0983,  1.0015, -1.4410,  0.8867, -0.9364,  0.5565],\n",
      "        [-0.7986, -1.0551, -1.0690, -0.4518,  1.1318,  1.2308,  1.2943],\n",
      "        [ 0.8707, -1.2232,  0.0219, -1.0712, -1.0084,  1.1639, -1.1096],\n",
      "        [ 1.0579, -1.0407, -1.0716, -0.3388, -0.7817, -0.6236,  1.4391],\n",
      "        [ 0.9188,  0.9736, -1.2876, -1.1246,  0.9118,  1.0198, -0.6442]])), ('output.bias', tensor([ 1.9662, -3.2039, -6.2933,  0.7052,  3.0020,  9.3696,  1.0143,  6.9622,\n",
      "        -6.7931, -1.0619, -9.3611, -2.9843, -0.7227,  6.2599,  3.1062, -1.9870]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "threshold = 39\n",
    "# modified_model_pth = f\"Hamming74/Result/Model/SLNN_decrease_hidden.weight_cpu/SLNN_model_hiddenlayer{threshold}_BER0.pth\"\n",
    "modified_model_pth = f\"Hamming74/Result/Model/SLNN_modified_neuron7_cpu_hidden.weight/SLNN_model_modified_hiddenlayer7_threshold{threshold}_BER0.pth\"\n",
    "print(\"modified model:\", torch.load(modified_model_pth))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T17:16:12.034932Z",
     "start_time": "2024-02-28T17:16:12.017478Z"
    }
   },
   "id": "a9876b58dfc0d255",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your input tensor\n",
    "input_tensor = torch.tensor([[ 0.0000,  0.0000,  0.7195,  0.0000,  0.0000,  0.7038,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000,  0.7659,  0.0000,  0.7190,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1143,  0.0000,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8998],\n",
    "        [ 0.0000,  1.2054,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "        [ 0.6529,  0.0000,  0.6603,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.6563,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "\n",
    "# Apply thresholding operation\n",
    "output_tensor = torch.where(input_tensor != 0, torch.tensor(1), torch.tensor(0))\n",
    "print(output_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T17:16:32.065665Z",
     "start_time": "2024-02-28T17:16:32.055763Z"
    }
   },
   "id": "68f0ffc4c3f39ba6",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5.0000e-04, 8.0000e-04, 3.4000e-03, 1.1000e-02, 1.2200e-02, 1.6000e-02,\n        1.6300e-02, 1.6700e-02, 1.7400e-02, 1.8000e-02, 1.9400e-02, 2.1700e-02,\n        2.3200e-02, 3.2400e-02, 3.2900e-02, 3.3600e-02, 3.5600e-02, 3.7200e-02,\n        3.7700e-02, 4.0800e-02, 4.1500e-02, 4.5600e-02, 4.7400e-02, 5.0100e-02,\n        5.0200e-02, 5.1700e-02, 5.3300e-02, 5.5000e-02, 6.0600e-02, 6.4300e-02,\n        6.5300e-02, 6.9500e-02, 7.2500e-02, 7.5500e-02, 7.6200e-02, 8.9600e-02,\n        9.0800e-02, 9.0900e-02, 9.3600e-02, 9.4700e-02, 9.8200e-02, 9.8900e-02,\n        1.0190e-01, 1.0270e-01, 1.0920e-01, 1.1360e-01, 1.1360e-01, 1.1490e-01,\n        1.1790e-01, 1.2090e-01, 1.2300e-01, 1.2360e-01, 1.2550e-01, 1.2860e-01,\n        1.2990e-01, 1.3030e-01, 1.3040e-01, 1.3200e-01, 1.3210e-01, 1.3250e-01,\n        1.3350e-01, 1.3420e-01, 1.3420e-01, 1.3450e-01, 1.3460e-01, 1.3690e-01,\n        1.3770e-01, 1.3850e-01, 1.4050e-01, 1.4070e-01, 1.4070e-01, 1.4110e-01,\n        1.4150e-01, 1.4150e-01, 1.4190e-01, 1.4300e-01, 1.4310e-01, 1.4460e-01,\n        1.4510e-01, 1.4530e-01, 1.4600e-01, 1.4820e-01, 1.4850e-01, 1.4860e-01,\n        1.5010e-01, 1.5020e-01, 1.5090e-01, 1.5140e-01, 1.5180e-01, 1.5200e-01,\n        1.5590e-01, 1.5590e-01, 1.5600e-01, 1.5790e-01, 1.5850e-01, 1.5880e-01,\n        1.5940e-01, 1.6100e-01, 1.6240e-01, 1.6260e-01, 1.6340e-01, 1.6380e-01,\n        1.6460e-01, 1.6550e-01, 1.6560e-01, 1.6590e-01, 1.6650e-01, 1.6810e-01,\n        1.6840e-01, 1.6840e-01, 1.6870e-01, 1.7050e-01, 1.7080e-01, 1.7150e-01,\n        1.7180e-01, 1.7310e-01, 1.7310e-01, 1.7330e-01, 1.7490e-01, 1.7500e-01,\n        1.7730e-01, 1.7860e-01, 1.7990e-01, 1.8010e-01, 1.8240e-01, 1.8410e-01,\n        1.8600e-01, 1.8710e-01, 1.8830e-01, 1.8910e-01, 1.8930e-01, 1.8970e-01,\n        1.9250e-01, 1.9290e-01, 1.9380e-01, 1.9460e-01, 1.9470e-01, 1.9690e-01,\n        2.0130e-01, 2.0560e-01, 2.0570e-01, 2.0640e-01, 2.0900e-01, 2.1260e-01,\n        2.1810e-01, 2.2650e-01, 2.2820e-01, 2.2930e-01, 2.3410e-01, 2.3960e-01,\n        2.4400e-01, 2.4510e-01, 2.4530e-01, 2.4790e-01, 2.4940e-01, 2.6980e-01,\n        2.7680e-01, 2.9490e-01, 3.7840e-01, 4.7520e-01, 6.7240e-01])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义张量\n",
    "tensor = torch.tensor([\n",
    "        0.2013, 0.0217, 0.2494, 0.1773, 0.0606, 0.2440, 0.0456,\n",
    "        0.0653, 0.0180, 0.0329, 0.2949, 0.1801, 0.2768, 0.1321,\n",
    "        0.0356, 0.0377, 0.0550, 0.1136, 0.4752, 0.1485, 0.1345,\n",
    "        0.2090, 0.0122, 0.1893, 0.0160, 0.1579, 0.0372, 0.3784,\n",
    "        0.0110, 0.6724, 0.0755, 0.0725, 0.0762, 0.0408, 0.0517,\n",
    "        0.2451, 0.0163, 0.2479, 0.2282, 0.0336, 0.2057, 0.0232,\n",
    "        0.2293, 0.0324, 0.1897, 0.1027, 0.1369, 0.0909, 0.2181,\n",
    "        0.0005, 0.0008, 0.0034, 0.0167, 0.0174, 0.0194, 0.0415, 0.0474, 0.0501,\n",
    "        0.0502, 0.0533, 0.0643, 0.0695, 0.0896, 0.0908, 0.0936, 0.0947, 0.0982,\n",
    "        0.0989, 0.1019, 0.1092, 0.1136, 0.1149, 0.1179, 0.1209, 0.1230, 0.1236,\n",
    "        0.1255, 0.1286, 0.1299, 0.1303, 0.1304, 0.1320, 0.1325, 0.1335, 0.1342,\n",
    "        0.1342, 0.1346, 0.1377, 0.1385, 0.1405, 0.1407, 0.1407, 0.1411, 0.1415,\n",
    "        0.1415, 0.1419, 0.1430, 0.1431, 0.1446, 0.1451, 0.1453, 0.1460, 0.1482,\n",
    "        0.1486, 0.1501, 0.1502, 0.1509, 0.1514, 0.1518, 0.1520, 0.1559, 0.1559,\n",
    "        0.1560, 0.1585, 0.1588, 0.1594, 0.1610, 0.1624, 0.1626, 0.1634, 0.1638,\n",
    "        0.1646, 0.1655, 0.1656, 0.1659, 0.1665, 0.1681, 0.1684, 0.1684, 0.1687,\n",
    "        0.1705, 0.1708, 0.1715, 0.1718, 0.1731, 0.1731, 0.1733, 0.1749, 0.1750,\n",
    "        0.1786, 0.1799, 0.1824, 0.1841, 0.1860, 0.1871, 0.1883, 0.1891, 0.1925,\n",
    "        0.1929, 0.1938, 0.1946, 0.1947, 0.1969, 0.2056, 0.2064, 0.2126, 0.2265,\n",
    "        0.2341, 0.2396, 0.2453, 0.2698\n",
    "])\n",
    "\n",
    "sorted_tensor, indices = tensor.sort()\n",
    "\n",
    "sorted_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T12:53:39.077606Z",
     "start_time": "2024-02-24T12:53:39.071396Z"
    }
   },
   "id": "d026bf1fcce61c25",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "    \\centering\n",
      "    \\begin{tabular}{|c|c|c|c|c|c|c|c|}\n",
      "        \\hline\n",
      "        \\textbf{Input Layer Neurons} & \\textbf{} & \\textbf{} & \\textbf{} & \\textbf{} & \\textbf{} & \\textbf{} & \\textbf{}\\\\\n",
      "        \\hline\n",
      "        1 & 0.5807 & 0.0 & 0.7195 & -0.5115 & -0.1748 & 0.7038 & -0.1317 \\\\\n",
      "        \\hline\n",
      "        2 & 0.1696 & 0.0 & -0.0854 & 0.7659 & -0.4676 & 0.719 & 0.343 \\\\\n",
      "        \\hline\n",
      "        3 & 0.0835 & 0.0884 & 0.1289 & -0.2663 & -1.1143 & -0.3482 & 0.3153 \\\\\n",
      "        \\hline\n",
      "        4 & 0.497 & 0.0 & -0.4501 & 0.0 & -0.3754 & 0.0885 & -0.8998 \\\\\n",
      "        \\hline\n",
      "        5 & 0.0 & 1.2054 & 0.1353 & 0.1299 & -0.1366 & 0.0732 & -0.0926 \\\\\n",
      "        \\hline\n",
      "        6 & 0.6529 & 0.0 & 0.6603 & 0.608 & -0.0896 & -0.5479 & 0.0 \\\\\n",
      "        \\hline\n",
      "        7 & -0.6563 & -0.0928 & 0.5427 & 0.294 & -0.3917 & 0.2601 & -0.624 \\\\\n",
      "        \\hline\n",
      "    \\end{tabular}\n",
      "    \\caption{SLNN, N = 7, normalized absolute weights}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# 数据列表\n",
    "data = [[ 0.5807,  0.0000,  0.7195, -0.5115, -0.1748,  0.7038, -0.1317],\n",
    "        [ 0.1696,  0.0000, -0.0854,  0.7659, -0.4676,  0.7190,  0.3430],\n",
    "        [ 0.0835,  0.0884,  0.1289, -0.2663, -1.1143, -0.3482,  0.3153],\n",
    "        [ 0.4970,  0.0000, -0.4501,  0.0000, -0.3754,  0.0885, -0.8998],\n",
    "        [ 0.0000,  1.2054,  0.1353,  0.1299, -0.1366,  0.0732, -0.0926],\n",
    "        [ 0.6529,  0.0000,  0.6603,  0.6080, -0.0896, -0.5479,  0.0000],\n",
    "        [-0.6563, -0.0928,  0.5427,  0.2940, -0.3917,  0.2601, -0.6240]]\n",
    "\n",
    "# 生成 LaTeX 代码\n",
    "latex_code = \"\\\\begin{table}[htbp]\\n\" \\\n",
    "             \"    \\\\centering\\n\" \\\n",
    "             \"    \\\\begin{tabular}{|c|c|c|c|c|c|c|c|}\\n\" \\\n",
    "             \"        \\\\hline\\n\" \\\n",
    "             \"        \\\\textbf{Input Layer Neurons} & \\\\textbf{} & \\\\textbf{} & \\\\textbf{} & \\\\textbf{} & \" \\\n",
    "             \"\\\\textbf{} & \\\\textbf{} & \\\\textbf{}\\\\\\\\\\n\" \\\n",
    "             \"        \\\\hline\\n\"\n",
    "\n",
    "# 遍历数据列表并生成表格行\n",
    "for i, row in enumerate(data):\n",
    "    latex_code += f\"        {i+1} & {' & '.join(map(str, row))} \\\\\\\\\\n\"\n",
    "    latex_code += \"        \\\\hline\\n\"\n",
    "\n",
    "# 添加表格结尾\n",
    "latex_code += \"    \\\\end{tabular}\\n\" \\\n",
    "              \"    \\\\caption{SLNN, N = 7, normalized absolute weights}\\n\" \\\n",
    "              \"\\\\end{table}\"\n",
    "\n",
    "# 输出 LaTeX 代码\n",
    "print(latex_code)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T14:39:55.995288Z",
     "start_time": "2024-02-25T14:39:55.990206Z"
    }
   },
   "id": "8972d4756cfb0b28",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e40424615971445"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
