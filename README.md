# Master Thesis: Improve Error Correction Code by Machine Learning and Deep Learning
Implemented in PyTorch.

### Implement the Soft-Decision Maximum Likelihood, Hard-Decsion Maximum Likelihood, and Belief propagation
Belief Propagation Reference: <br>
D. E. Hocevar, "A reduced complexity decoder architecture via layered decoding of LDPC codes," IEEE Workshop on Signal Processing Systems, 2004. SIPS 2004., Austin, TX, USA, 2004, pp. 107-112, doi: 10.1109/SIPS.2004.1363033.

### Implement Neural Network Decoder:
Neural Network Decoder Reference: <br>
C. T. Leung, R. V. Bhat and M. Motani, "Low Latency Energy-Efficient Neural Decoders for Block Codes," in IEEE Transactions on Green Communications and Networking, vol. 7, no. 2, pp. 680-691, June 2023, doi: 10.1109/TGCN.2022.3222961.

### Error Correction Code Transformer:
Transformer Reference: <br>
Yoni Choukroun and Lior Wolf. Error correction code transformer. arXiv preprint arXiv:2203.14966, 2022.

### Cross-attention Transformer for Error Correcting Codes:
Cross-attention Transformer Reference: <br>
Seong-Joon Park and Hee-Youl Kwak and Sang-Hyo Kim and Yongjune Kim and Jong-Seon No. CrossMPT: Cross-attention Message-Passing Transformer for Error Correcting Codes. arXiv preprint arXiv:2405.01033, 2024

To train the model faster, I used early stopping.

### Addition: In Good First Issue, I mentioned some common bugs and solutions in Belief Pragation, BER and BLER.
